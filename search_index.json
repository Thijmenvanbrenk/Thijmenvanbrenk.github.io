[["index.html", "Thijmens portfolio Section 1 Introduction 1.1 Personality 1.2 Reproducability 1.3 Extra skills", " Thijmens portfolio Thijmen van Brenk 2022-06-12 Section 1 Introduction Welcome to my portfolio In this github page I will show what I have learned in the past 5 months. On the left side you can see different subjects I have worked on. You can access them easily by clicking on them. If you want to know what every section entails, without having to click every one of them, there is a small description of every section below. 1.1 Personality  CV: In this section you can find my CV with my education and work skills. (in dutch) 1.2 Reproducability In the reproducability section I will be handling topics that have to do with open science and reproducability.  Reproducing skills: In this section you can see how I used a dataset from a published paper and how I was able to reproduce their results.  Grading reproducability: In this section you can see how I graded a paper on the criteria for reproducability. And the code from another paper was graded on readability and reproducability.  Organization: In this section you can see how I have stored my files for a previous project. 1.3 Extra skills In the extra skills section I will be showing you some of my extra skills like different programming languages.  Using references: In this section you can see how I have written an introduction using multiple references.  Using parameters: In this section you can see how I made use of multiple parameter settings to make creating graphs easier.  Using databases: In this section you will see how I store data in a database and how I extract specific values from it.  Creating a package: In this section you can see one of my created packages with all the attributes.  Making a phylogeny tree: In this section you can see how I went through a workflow of creating a phylogeny tree, how to use paramaters to show different values and how I made it reproducible by integrating it into an R-shiny "],["personality-1.html", "Section 2 Personality 2.1 CV", " Section 2 Personality 2.1 CV Below you can see my CV Because making the CV uses special headers its not possible to knit it into bookdown, thats why this is just the picture of the CV. the orginal code can be found in this repository at: data/CV_thijmenvanbrenk.Rmd "],["reproducability-1.html", "Section 3 Reproducability 3.1 Reproducing data from a published paper 3.2 Checking reproducability for published papers. 3.3 Organisation of my files", " Section 3 Reproducability 3.1 Reproducing data from a published paper Here I am showing you how I am able to reproduce results from a published paper. The data used in this assignment comes from (van der Voet et al. 2021) library(tidyverse) library(here) library(readxl) library(rbbt) library(RColorBrewer) offspring &lt;- read_excel(here(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;), sheet = 1) # we want to see if the data for the experimental conditions have been imported correctly offspring %&gt;% select(c(&quot;expType&quot;, &quot;RawData&quot;, &quot;compName&quot;, &quot;compConcentration&quot;)) ## # A tibble: 360 x 4 ## expType RawData compName compConcentration ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 experiment 44 2,6-diisopropylnaphthalene 4.99 ## 2 experiment 37 2,6-diisopropylnaphthalene 4.99 ## 3 experiment 45 2,6-diisopropylnaphthalene 4.99 ## 4 experiment 47 2,6-diisopropylnaphthalene 4.99 ## 5 experiment 41 2,6-diisopropylnaphthalene 4.99 ## 6 experiment 35 2,6-diisopropylnaphthalene 4.99 ## 7 experiment 41 2,6-diisopropylnaphthalene 4.99 ## 8 experiment 36 2,6-diisopropylnaphthalene 4.99 ## 9 experiment 40 2,6-diisopropylnaphthalene 4.99 ## 10 experiment 38 2,6-diisopropylnaphthalene 4.99 ## # ... with 350 more rows # as we can see, the rawdata should have been an integer, the compname and expType should have been a factor and the compconcentration should have been a double. lets change that offspring$RawData &lt;- as.integer(offspring$RawData) offspring$compName &lt;- as.factor(offspring$compName) offspring$expType &lt;- as.factor(offspring$expType) offspring_tidy &lt;- offspring offspring_tidy$compConcentration &lt;- as.numeric(offspring_tidy$compConcentration) # one of the values in compconcentration is accidentally classified as a character in excel and has now turned into a NA value, we will change this value manually. character_placement &lt;- which(is.na(offspring_tidy$compConcentration)) character_value &lt;- offspring$compConcentration[character_placement] %&gt;% str_replace(&quot;,&quot;, &quot;.&quot;) %&gt;% parse_number() offspring_tidy$compConcentration[character_placement] &lt;- character_value # lets check one last time if the data types are correct. offspring %&gt;% select(c(&quot;RawData&quot;, &quot;compName&quot;, &quot;compConcentration&quot;)) ## # A tibble: 360 x 3 ## RawData compName compConcentration ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 ## 2 37 2,6-diisopropylnaphthalene 4.99 ## 3 45 2,6-diisopropylnaphthalene 4.99 ## 4 47 2,6-diisopropylnaphthalene 4.99 ## 5 41 2,6-diisopropylnaphthalene 4.99 ## 6 35 2,6-diisopropylnaphthalene 4.99 ## 7 41 2,6-diisopropylnaphthalene 4.99 ## 8 36 2,6-diisopropylnaphthalene 4.99 ## 9 40 2,6-diisopropylnaphthalene 4.99 ## 10 38 2,6-diisopropylnaphthalene 4.99 ## # ... with 350 more rows # they are so we can now use the data for further analysis offspring_tidy %&gt;% ggplot(aes(x = log10(compConcentration + 0.0001), y = RawData)) + geom_jitter(aes(shape = expType, colour = compName), width = .1) + labs(title = &quot;Amount of offspring from C. elegans incubated in different substances&quot;, subtitle = &quot;Experiment data from (van der Voet et al. 2021)&quot;, x = &quot;Log 10 of compound concentration&quot;, y = &quot;Amount of offspring per C. elegans&quot;, colour = &quot;Compound name&quot;, shape = &quot;Experiment type&quot;) + scale_shape_discrete(labels = c(&quot;Negative control&quot;, &quot;Positive control&quot;, &quot;Vehicle A control&quot;, &quot;Experiment&quot;)) + scale_colour_brewer(palette = &quot;Dark2&quot;) + theme_classic() The positive control of this experiment is Ethanol and the negative control is no added substance. To analyze this experiment I would follow these steps. 1. Making a new column which shows which condition every worm is located in. (for example, group1 would consist of 2,6-diisopropylnaphthalene with a concentration of 4.99 nM, etc.) 2. Checking normality for every condition. NORMALLY DISTRIBUTED DATA: 3. Perform ANOVA. with post-hoc tests and check if they differ from the control. NOT NORMALLY DISTRIBUTED DATA: 3. Perform Kruskal - Wallis test. 4. To visualize this difference, make a smoothed line graph for every the mean of every concentration per substance. 5. Compare these graphs with each other. For now lets normalize the values by taking the average of the negative control and using that as 100%. That way its easier to see the difference between compounds. normalized_value &lt;- offspring_tidy %&gt;% group_by(compName) %&gt;% filter(compName == &quot;S-medium&quot;) %&gt;% summarise(mean = mean(RawData, na.rm = T)) offspring_tidy &lt;- offspring_tidy %&gt;% mutate(normalized_offspring = RawData/normalized_value$mean) offspring_tidy %&gt;% ggplot(aes(x = log10(compConcentration + 0.0001), y = normalized_offspring)) + geom_jitter(aes(shape = expType, colour = compName), width = .1) + labs(title = &quot;Amount of offspring from C. elegans incubated in different substances&quot;, subtitle = &quot;Experiment data from (van der Voet et al. 2021)&quot;, x = &quot;Log 10 of compound concentration&quot;, y = &quot;Normalized offspring amount by mean of negative control&quot;, colour = &quot;Compound name&quot;, shape = &quot;Experiment type&quot;) + scale_shape_discrete(labels = c(&quot;Negative control&quot;, &quot;Positive control&quot;, &quot;Vehicle A control&quot;, &quot;Experiment&quot;)) + scale_colour_brewer(palette = &quot;Dark2&quot;) + theme_classic() 3.2 Checking reproducability for published papers. In this assignment, this study (Strobl et al. 2020) will be graded on the criteria for reproducibility. And this study (Brewer, Robey, and Unsworth 2021) will be graded on code readability and reproducibility. 3.2.1 Pesticide influence on consumption rate and survival for bees. Introduction of the paper The use of pesticides is one of the main reasons of loss of biodiversity, and the combination of multiple pesticides could even make this worse. In this experiment it is investigated what the sublethal (food consumption) and the lethal (survival) effects of pesticides are on adult female solitary bees, Osmia bicornis. To perform these tests, female solitary bees were divided into 4 groups:  pesticide free (control)  Herbicide  Pesticide  Combined (both herbicide and pesticide) Their consumption rate and longevity were measured and the data from these two variables are used for analysis. There is no significant difference in survival and consumption between te different groups. there is however a significant positive correlation between the consumption rate and the longevity of these bees. Now we can grade this paper on the criteria for transparancy. you can find the results in the table below. Transparancy criteria grading Transparancycriteria Grading Study purpose TRUE Dataavailability FALSEOnly part of the data isavailable Data location At the beginning/at the end Studylocation TRUEmaterials/methods Authorreview Location and emailare present at the top Ethicsstatement FALSE Fundingstatement TRUE Codeavailability TRUE The part of the data that is available can be accessed through this directory: data/insects-957898-supplementary.xlsx 3.2.2 impact of analysis decisions for episodic memory and retrieval practices. We will solely focus on the code of this paper to see:  If the code can be understood easily.  If I can reproduce one of the figures.  If there are any bugs/flaws in the code. The code is available in this website The code has been copied to a new Rmd file in this repository under the name _analysis_decisions_code.Rmd. The data has been downloaded and is available in this repository under the name data/AllDataRR.csv Changes made:  Changed the directory in line 11 so it retrieved the data used from this study.  Installed the packages in line 19 and line 180 by deleting the # in front of those lines. First impression:  (+) Every test is in different chunks which makes readability easier.  (+) Clear comments on what is happening.  (+) Easy to understand code  (-) Chunks dont have names.  (-) The individual results are far away from each other.  (-) The same tests are are performed multiple times, making a function would make chances of mistakes less likely What this code is trying to achieve The first part of the code for this experiment is looking for the correlation between individual and different studies (line 24-174) The second part of the code for this experiment is looking at a correlation between the retrieval practice effect and the EM ability with the help of a graph. there are 2 graphs, one where everything is mean centered and one where it isnt. Final judgement: (grading goes from 1-5(1 very hard/bad- 5 very easy/good))  Readability = 4  Reproducability = 5  Efficiency = 2 3.3 Organisation of my files Storing files in a way everyone knows where they can find everything makes it easier for your colleagues or complete strangers to find what they are looking for. Down below you can see the file structure from one of my previous projects. As you can see all the files are stored in a similar way and can be easily found. References "],["extra-skills-1.html", "Section 4 Extra skills 4.1 Writing an introduction using Zotero for references 4.2 Creating paramaters for different data inputs 4.3 Using SQL to store data 4.4 Creating a package 4.5 Creating an epidemiology map", " Section 4 Extra skills In this section of my portfolio I will show you some of the extra skills I have developed during my data science minor. 4.1 Writing an introduction using Zotero for references Writing research papers is a pretty important for the research field, so using references to other papers is crucial for writing a good introduction. Below here you can see an introduction to my project about liquid biopsies, it makes use of multiple references. Neuroblastoma is the fourth most common tumor in children and presents itself as either a low-risk neuroblastoma or a high-risk neuroblastoma. Determining which risk neuroblastoma is present a tumor biopsy is necessary. (Weiser et al. 2019) however these biopsies are very invasive and can only be done a few times even tho the tumor keeps mutating. To counter this problem researchers have become more and more interested in liquid biopsies to be able to track the mutations in the tumor. This is possible because the tumor often excretes DNA into the blood stream which is then used for whole-exome sequencing (WES). these results can be compared with the DNA of the tumor to show how the tumor evolves over time (Chicard et al. 2018). because the tumor cells have different properties depending on what part of the tumor they are on it is difficult to show all the mutations based on 1 tumor biopsy as that can have a bias for only that specific part of the tumor where the biopsy was taken from. Liquid biopsies also help with this problem as they sequence all the DNA that has been excreted by the tumor, this makes it possible to spot different mutations in both tumor DNA and cell free DNA (cf-DNA). (Van Paemel et al. 2022) The type of mutations that get the most attention are the copy number variations/aberrations (CNV/CNA) these CNVs can either be very small with just a few kilobases or very big where they cover the whole chromosome. The CNVs are very important as they can give an identification on how pathogenic the tumor is based on the genes it contains, the position of the CNV and the size of the CNV (Riggs et al. 2020). Some hospitals sadly dont have easy ways to analyse the data from all these patients because they do not have the experience with data analysis. This makes the process of analyzing the data a slow and tedious task. Even tho it would be extremely beneficial for the hospitals without data scientists to have the programs available to analyse these results quickly (Valsesia et al. 2013), this takes away the time consuming task of having to analyze every file manually which gives them time to focus on more important things like treating the patient. Sharing these results to other hospitals is equally as important because there is not nearly enough data available to say with certainty how dangerous certain tumors are and if the tumors have been fully removed. With all the data combined the research towards liquid biopsies can evolve quickly making diagnoses easier and more reliable In this project we will analyse the tumor DNA and the cfDNA created by WES and will make it reproducible so that Princess maxima centre can easily analyse the data for all their patients. To accomplish this we will mostly focus on:  Giving all the CNVs different IDs so they can be easily distinguished from each other.  Showing which cytogenetic band the CNV falls in to.  Making an interactive plot to look up genes easily.  Automatically filtering genes that indicate high risk neuroblastomas.  Making a high throughput version so that it will analyse multiple datasets at the same time without having to manually insert all the data sets  Getting these results quickly and easily so the researcher does not have to focus on how to analyze the data. 4.2 Creating paramaters for different data inputs To show my ability to use paramaters I will be using data from the ECDC. the data is available in this repository under data/COVID_cases_31_05_2022 # loading in data cases &lt;- read.csv(&quot;data/COVID_cases_31_05_2022.csv&quot;) # filtering the params used cases_filtered &lt;- cases %&gt;% dplyr::filter(countriesAndTerritories == params$country, year == params$year, month &gt;= params$period_start, month &lt;= params$period_end) # telling R the dateRep column is a date cases_filtered$dateRep &lt;- as.Date(cases_filtered$dateRep, format = &quot;%d/%m/%Y&quot;) # making a graph for cases cases_graph &lt;- cases_filtered %&gt;% ggplot(aes(x = dateRep, y = cases)) + geom_point(size = .5) + geom_line() + labs(title = paste(&quot;Covid related cases from month&quot;, params$period_start, &quot;to&quot;, params$period_end, &quot;in&quot;, params$year, &quot;for&quot;, params$country), x = &quot;Month&quot;, y = &quot;Covid related cases&quot;) + theme_classic() ggplotly(cases_graph) # making a graph for deaths deaths_graph &lt;- cases_filtered %&gt;% ggplot(aes(x = dateRep, y = deaths)) + geom_point(size = .5) + geom_line() + labs(title = paste(&quot;Covid related deaths from month&quot;, params$period_start, &quot;to&quot;, params$period_end, &quot;in&quot;, params$year, &quot;for&quot;, params$country), x = &quot;Month&quot;, y = &quot;Covid related deaths&quot;) + theme_classic() ggplotly(deaths_graph) If you want to recreate these graphs with different parameters, clone this repository and use put this command in the console (with your own params ofcourse): bookdown::render_book(params = list(country = &quot;Netherlands&quot;, year = 2021, period_start = 5, period_end = 10)) 4.3 Using SQL to store data Databases are an important feature of data science (it is even in the name), thats why being able to request data from a database is equally as important as actually analyzing the data. First we need to make a new database, this has been done with the use of DBeaver and the code for creating the database can be found below. SQL script of database creation after creating the database we need to make a connection with DBeaver. to reproduce this: * make a new database in dbeaver called dengue_flu_data. * insert your own password in the yaml header of this file in my repository. IMPORTANT MESSAGE: The source of the data used for these analyses is NOT available anymore, so the conclusions that have been formed based on the graphs are conclusions based on data that can not be confirmed anymore. library(DBI) con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;dengue_flu_data&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;, user = &quot;postgres&quot;, password = params$password) library(tidyverse) library(dslabs) library(car) gapminder &lt;- gapminder dengue_data &lt;- read_csv(&quot;data/dengue_data.csv&quot;, skip = 11) flu_data &lt;- read_csv(&quot;data/flu_data.csv&quot;, skip = 11) dengue_data %&gt;% head(5) ## # A tibble: 5 x 11 ## Date Argentina Bolivia Brazil India Indonesia Mexico Philippines ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2002-12-29 NA 0.101 0.073 0.062 0.101 NA NA ## 2 2003-01-05 NA 0.143 0.098 0.047 0.039 NA NA ## 3 2003-01-12 NA 0.176 0.119 0.051 0.059 0.071 NA ## 4 2003-01-19 NA 0.173 0.17 0.032 0.039 0.052 NA ## 5 2003-01-26 NA 0.146 0.138 0.04 0.112 0.048 NA ## # ... with 3 more variables: Singapore &lt;dbl&gt;, Thailand &lt;dbl&gt;, Venezuela &lt;dbl&gt; flu_data %&gt;% head(5) ## # A tibble: 5 x 30 ## Date Argentina Australia Austria Belgium Bolivia Brazil Bulgaria Canada ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2002-12-29 NA NA NA NA NA 174 NA NA ## 2 2003-01-05 NA NA NA NA NA 162 NA NA ## 3 2003-01-12 NA NA NA NA NA 174 NA NA ## 4 2003-01-19 NA NA NA NA NA 162 NA NA ## 5 2003-01-26 NA NA NA NA NA 131 NA NA ## # ... with 21 more variables: Chile &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, ## # Hungary &lt;dbl&gt;, Japan &lt;dbl&gt;, Mexico &lt;dbl&gt;, Netherlands &lt;dbl&gt;, ## # `New Zealand` &lt;dbl&gt;, Norway &lt;dbl&gt;, Paraguay &lt;dbl&gt;, Peru &lt;dbl&gt;, ## # Poland &lt;dbl&gt;, Romania &lt;dbl&gt;, Russia &lt;dbl&gt;, `South Africa` &lt;dbl&gt;, ## # Spain &lt;dbl&gt;, Sweden &lt;dbl&gt;, Switzerland &lt;dbl&gt;, Ukraine &lt;dbl&gt;, ## # `United States` &lt;dbl&gt;, Uruguay &lt;dbl&gt; As we can see both the flu and dengue data are not tidy ( &gt; 1 observations per row). This can easily be fixed with pivot_longer() dengue_tidy &lt;- dengue_data %&gt;% pivot_longer(cols = Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;Dengue_cases&quot;) dengue_tidy %&gt;% head(5) ## # A tibble: 5 x 3 ## Date country Dengue_cases ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Bolivia 0.101 ## 3 2002-12-29 Brazil 0.073 ## 4 2002-12-29 India 0.062 ## 5 2002-12-29 Indonesia 0.101 flu_tidy &lt;- flu_data %&gt;% pivot_longer(cols = Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;Flu_cases&quot;) flu_tidy %&gt;% head(5) ## # A tibble: 5 x 3 ## Date country Flu_cases ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002-12-29 Argentina NA ## 2 2002-12-29 Australia NA ## 3 2002-12-29 Austria NA ## 4 2002-12-29 Belgium NA ## 5 2002-12-29 Bolivia NA # data is now tidy If we want to eventually merge these 3 data files together we need to make the date and country variables equal:  Date from flu and dengue data need to be split into year, month, day.  Country needs to be of class factor. dengue_tidy$country &lt;- as.factor(dengue_tidy$country) flu_tidy$country &lt;- as.factor(flu_tidy$country) dengue_tidy &lt;- dengue_tidy %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), convert = T, sep = &quot;-&quot;) flu_tidy &lt;- flu_tidy %&gt;% separate(Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), convert = T, sep = &quot;-&quot;) We will store this data so we always have our tidy versions by hand. dengue_tidy %&gt;% write.csv(&quot;data/denguetidy.csv&quot;) dengue_tidy %&gt;% write_rds(&quot;data/denguetidy.rds&quot;) flu_tidy %&gt;% write.csv(&quot;data/flutidy.csv&quot;) flu_tidy %&gt;% write_rds(&quot;data/flutidy.rds&quot;) gapminder %&gt;% write.csv(&quot;data/gapminder.csv&quot;) gapminder %&gt;% write_rds(&quot;data/gapminder.rds&quot;) Now we can insert these dataframes into SQL. dbWriteTable(con, &quot;dengue&quot;, dengue_tidy) dbWriteTable(con, &quot;flu&quot;, flu_tidy) dbWriteTable(con, &quot;gapminder&quot;, gapminder) To check if everything imported correctly we will go to DBeaver and inspect the data. We can also get the datasets back and check them using R dengueSQL &lt;- dbReadTable(con, &quot;dengue&quot;) fluSQL &lt;- dbReadTable(con, &quot;flu&quot;) gapminderSQL &lt;- dbReadTable(con, &quot;gapminder&quot;) fluSQL %&gt;% head(5) ## year month day country Flu_cases ## 1 2002 12 29 Argentina NA ## 2 2002 12 29 Australia NA ## 3 2002 12 29 Austria NA ## 4 2002 12 29 Belgium NA ## 5 2002 12 29 Bolivia NA gapminderSQL %&gt;% head(5) ## country year infant_mortality life_expectancy fertility ## 1 Albania 1960 115.40 62.87 6.19 ## 2 Algeria 1960 148.20 47.50 7.65 ## 3 Angola 1960 208.00 35.98 7.32 ## 4 Antigua and Barbuda 1960 NA 62.97 4.43 ## 5 Argentina 1960 59.87 65.39 3.11 ## population gdp continent region ## 1 1636054 NA Europe Southern Europe ## 2 11124892 13828152297 Africa Northern Africa ## 3 5270844 NA Africa Middle Africa ## 4 54681 NA Americas Caribbean ## 5 20619075 108322326649 Americas South America It seems like inserting it into DBeaver and pulling it back changes the data classes for every variable. This is not a big problem, we just have to make sure to change the classes of certain values whenever we import something from DBeaver. We can now combine these 3 dataframes into 1 big dataframe based on country and year. flu_dengue_combined &lt;- full_join(dengueSQL, fluSQL, by = c(&quot;country&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) # lets check how many years all 3 dataframes are covering flu_dengue_combined$year %&gt;% min() ## [1] 2002 flu_dengue_combined$year %&gt;% max() ## [1] 2015 # flu and dengue have a range from 2002 to 2015 gapminderSQL$year %&gt;% min() ## [1] 1960 gapminderSQL$year %&gt;% max() ## [1] 2016 # gapminder has a range from 1960 to 2016. # we dont need all those years so we will cut those out gapminder_filtered &lt;- gapminderSQL %&gt;% filter(year &gt;= 2002) %&gt;% filter(year &lt;= 2015) # lets update the gapminder data in SQL so it only contains this dataframe instead of the big one because it is not required dbWriteTable(con, &quot;gapminder&quot;, gapminder, overwrite = T) # now we can combine all the data frames combined_all &lt;- full_join(flu_dengue_combined, gapminder_filtered, by = c(&quot;country&quot;, &quot;year&quot;)) dbWriteTable(con, &quot;gap_flu_den&quot;, combined_all) Now that we have a table with all the data combined we can ask SQL to extract specific data to do some analysis on. Lets say we want to know if there is a difference in the amount of flu cases of brazil and argentina in 2007. # getting all the relevant data arg_bra_flucases &lt;- dbGetQuery(con, &quot;SELECT \\&quot;month\\&quot;, country, gfd.\\&quot;Flu_cases\\&quot;, population FROM gap_flu_den gfd WHERE country IN (\\&#39;Argentina\\&#39;, \\&#39;Brazil\\&#39;) AND \\&quot;year\\&quot; = 2007;&quot;) # because brazil has a much higher population we will use the cases per 100.000 citizens. arg_bra_flucases &lt;- arg_bra_flucases %&gt;% mutate(per_100.000 = Flu_cases * 100000 / population) summary_arg_bra_flu &lt;- arg_bra_flucases %&gt;% group_by(month, country) %&gt;% summarise(total = sum(per_100.000)) ## `summarise()` has grouped output by &#39;month&#39;. You can override using the ## `.groups` argument. summary_arg_bra_flu %&gt;% ggplot(aes(x = month.name[month], y = total, colour = country)) + geom_point() + geom_path(aes(group = country)) + labs(title = &quot;Flu cases per 100.000 citizens in argentina and brazil&quot;, subtitle = &quot;Data from 2007&quot;, x = &quot;Month of the year&quot;, y = &quot;Cases per 100.000 citizens&quot;) + scale_x_discrete(limits = month.name) + theme_classic() + theme(axis.text.x = element_text(angle = 60, vjust = 0.7)) # on first glance Argentina seems to have more cases per 100.000 citizens, but just to make sure we will do a two sample t-test summary_arg_bra_flu %&gt;% group_by(country) %&gt;% summarise(p.value.normality = shapiro.test(total)$p.value) ## # A tibble: 2 x 2 ## country p.value.normality ## &lt;chr&gt; &lt;dbl&gt; ## 1 Argentina 0.210 ## 2 Brazil 0.312 # both have p &gt; 0.05, so they are normally distributed, now we can continue with levennes test. leveneTest(summary_arg_bra_flu$total, as.factor(summary_arg_bra_flu$country), center = mean)$P ## [1] 0.0001063121 NA # p &lt; 0.05, so they do not have equal variance. we can now perform the t test t.test(formula = summary_arg_bra_flu$total ~ summary_arg_bra_flu$country, paired = F, var.equal = F)$p.value ## [1] 0.0003999675 # p &lt; 0.001, There is a significant difference between the total amount of cases per 100.000 citizens in Argentina and Brazil. This had some interesting results. lets try something different this time. Lets see if some south-east asian countries have reduced their dengue infections between 2002 - 2015. SEAsia_dengue_cases &lt;- dbGetQuery(con, &quot;SELECT gfd.\\&quot;year\\&quot;, gfd.\\&quot;Dengue_cases\\&quot;, country FROM gap_flu_den gfd WHERE country IN (\\&#39;Singapore\\&#39;, \\&#39;Indonesia\\&#39;, \\&#39;Thailand\\&#39;, \\&#39;Philippines\\&#39;)&quot;) summary_SEAsia_den_cases &lt;- SEAsia_dengue_cases %&gt;% group_by(year, country) %&gt;% summarize(total = sum(Dengue_cases)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. summary_SEAsia_den_cases %&gt;% ggplot(aes(x = year, y = total, colour = country)) + geom_point() + geom_line() + scale_x_continuous(breaks = seq(2002, 2015, by = 2)) + labs(title = &quot;Total dengue cases per year&quot;, x = &quot;Year&quot;, y = &quot;Total cases per year&quot;) + theme_classic() ## Warning: Removed 9 rows containing missing values (geom_point). ## Warning: Removed 9 row(s) containing missing values (geom_path). The dengue cases seem to stay mostly stagnant with a few peaks here and there for every country, but in 2015 all dengue cases seem to have dropped which could be a good sign. Now lets take a look at one last graph. It is always said that the flu peak is always during winter because the cold makes it easier for the influenza virus to infect people. lets check this for ourselves by checking the flu incidence for the year 2009 in 5 northern hemisphere countries. flu_incidence_northern &lt;- dbGetQuery(con, &quot;SELECT country, \\&quot;month\\&quot;, gfd.\\&quot;Flu_cases\\&quot; FROM gap_flu_den gfd WHERE country IN (\\&#39;Netherlands\\&#39;, \\&#39;Norway\\&#39;, \\&#39;France\\&#39;, \\&#39;Sweden\\&#39;, \\&#39;Switzerland\\&#39;) AND \\&quot;year\\&quot; = 2009&quot;) summary_flu_inc_north &lt;- flu_incidence_northern %&gt;% group_by(month, country) %&gt;% summarize(total = sum(Flu_cases)) ## `summarise()` has grouped output by &#39;month&#39;. You can override using the ## `.groups` argument. summary_flu_inc_north %&gt;% ggplot(aes(x = month.name[month], y = total, group = country, fill = country)) + geom_col(position = position_dodge()) + labs(title = &quot;Flu incidence for different countries&quot;, subtitle = &quot;Data from 2009&quot;, x = &quot;Month&quot;, y = &quot;Flu incidence&quot;) + scale_x_discrete(limits = month.name) + theme_classic() + theme(axis.text.x = element_text(angle = 60, vjust = 0.8)) As we can see in this graph, most flu incidence occurs during the winter so saying that people get sick from the flu in the winter would be correct. 4.4 Creating a package Creating a package can save people allot of time, because they dont have to write the same piece of code multiple times. That is why having the ability to make a package is very important as a data scientist. Because I am obsessed with bees I decided to make my life a bit easier, thats why I created a package that will perform my calculations for me so I can save myself some time and make my life just that extra bit easier. You can find this package by clicking this link. You can download the package by using these two lines. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;thijmenvanbrenk/beecalculator&quot;) By using help(package = &quot;beecalculator&quot;) you will get a nice overview of the available data and functions available in this package. To get a detailed description of what everything does use browseVignettes(&quot;beecalculator&quot;) to see all the possibilities with this package. 4.5 Creating an epidemiology map With the infrastructure we have created to make it easier for people to connect, we have also made the infrastructure for diseases to spread. Many of these diseases cause allot of trouble and can weaken whole societies, just take a look at SARS-COV-19. To figure out how these diseases spread is essential to figure out how a pathogen behaves. Figuring this out gives us the possibility to take precautions so it does not happen again, as can be seen by this report from the CDC. In my future I want to be able to process epidemiological data to figure out where a disease originated, how it spread to different people and make this data easily readable for non data scientists with the help of shiny. To learn these skills I have made a small plan for a few steps I want to go through: 1. Find a suitable outbreak with available data. 2. Process the data to get simple phylogenetic trees. 3. Add regions to these trees. 4. Create a shiny to make it easy for others to create these graphs 5. Give more parameters, like showing only specific regions or adding different species. (optional depending on time) 4.5.1 Finding an outbreak To be able and actually to create phylogenetic trees I need to have some data available to me. I dont want a massive amount of data like from the covid pandemic because this would be too much data to process while this is only for my own learning purpose. the NCBI has a very detailed database of different virus variation, for this exercise I will be using the data from the MERS-Cov outbreak (2013-2019). If you want to download this data for yourself go to THIS SITE and follow the following steps: 1. Go to MERS coronavirus 2. Select nucleotide sequence type. 3. Select Human host. 4. Select S genome region. 5. Click additional filters, type complete cds and switch to definition lines. 6. Click Add query. 7. Click Show results. (this will redirect you to a new screen) 8. Make sure only the S genome regions are selected! 9. Click Customize label, make sure the label only contains {accession}. (this makes it easier to see later on) 10. Click Download with as download option Nucleotide (FASTA). 11. Click Download but now with as download option Result set (CSV). (This is the metadata) You now have 2 files: 1. The nucleotide sequence of all samples taken from human hosts with the accession number. 2. The metadata that shows all the information for each accession number. I will only be working with these 2 datafiles. 4.5.2 Creating Phylogenetic trees For learning how to create these phylogenetic trees I have taken inspiration from a small tutorial on how to create Phylogenetic trees. You can find this tutorial Here. After getting an idea of how to make these trees I have taken further inspiration from ggtree and all the help for that on google. # now we need to load in the nucleotides dna &lt;- readDNAStringSet(here(&quot;data/MERS_nucleotides.fa&quot;)) dna ## DNAStringSet object of length 168: ## width seq names ## [1] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KT805988 ## [2] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KT805968 ## [3] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KT357808 ## [4] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KM027279 ## [5] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KT805976 ## ... ... ... ## [164] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KM027264 ## [165] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA MH978886 ## [166] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KT806039 ## [167] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KM027269 ## [168] 4062 ATGATACACTCAGTGTTTCTAC...GCATAAGGTTCATGTTCACTAA KY673146 # and the metadata metadata &lt;- read.csv(here(&quot;data/MERS_annotation.csv&quot;)) metadata %&gt;% head(5) %&gt;% knitr::kable() accession length genome_region host country isolation collection_date release_date name KJ782549 4062 S Homo sapiens Greece oronasopharynx 2014/04/18 2014/05/13 Middle East respiratory syndrome coronavirus strain Greece-Saudi Arabia_2014 S protein (S) gene, complete cds KF811036 4062 S Homo sapiens Tunisia blood 2013/05/08 2014/05/19 Middle East respiratory syndrome coronavirus strain Tunisia-Qatar_2013 spike protein gene, complete cds KM027263 4062 S Homo sapiens Saudi Arabia 2014 2014/11/12 Middle East respiratory syndrome coronavirus isolate Jeddah_C7058/KSA/2014 spike protein (S) gene, complete cds KM027264 4062 S Homo sapiens Saudi Arabia 2014 2014/11/12 Middle East respiratory syndrome coronavirus isolate Jeddah_C7209/KSA/2014 spike protein (S) gene, complete cds KM027265 4062 S Homo sapiens Saudi Arabia 2014 2014/11/12 Middle East respiratory syndrome coronavirus isolate Jeddah_C7311/KSA/2014 spike protein (S) gene, complete cds # we only want the years from the metadata so lets extract those metadata &lt;- metadata %&gt;% separate(collection_date, into = &quot;collection_date&quot;, sep = 4) metadata$collection_date &lt;- as.numeric(metadata$collection_date) # the DNA is loaded in nicely with the exact amount of downloaded sequences. # there is no multiple sequence alignment necessary for these sequences, this is because they all come from the same part of the DNA. dna &lt;- as.DNAbin(dna) # sequence has to be the correct class # to create a phylogenetic tree we need to calculate the distance between all the sequences # there are many different models to choose from, and it was not easy to choose out of all the options because there are allot of upsides and allot of downsides to all the possible methods. for this time I have chosen to use the Tamaru and Nei methode (TN93). Because they take into account that its different to swap from A-G then C-T and vice versa. dna_distance &lt;- dist.dna(dna, model = &quot;TN93&quot;) # lets start by making a very simple neighbour joining phylogenetic tree. nj_tree &lt;- bionj(dna_distance) plot(nj_tree, cex = .4) # this is the most simplistic phylogenetic tree to be made and just shows the relation between the different samples. # now lets change it up so it isnt as clutured and make it give the information we actually want # we can also make it our tree rooted by adding the first sample ever taken nj_rooted &lt;- root(nj_tree, 1) %&gt;% ladderize() # performing a bootstrap will not only tell us how good our lines are, it also give us the option to collapse some of them # we have to load the dna back with another method because otherwise boot.phylo cannot recognize it for some reason dna_boots &lt;- fasta2DNAbin(here(&quot;data/MERS_nucleotides.fa&quot;)) ## ## Converting FASTA alignment into a DNAbin object... ## ## ## Finding the size of a single genome... ## ## ## genome size is: 4,062 nucleotides ## ## ( 60 lines per genome ) ## ## Importing sequences... ## .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ## Forming final object... ## ## ...done. bootstrap &lt;- boot.phylo(nj_rooted, dna_boots, function(e) {root(bionj(dist.dna(e, model = &quot;TN93&quot;)), 1)}) ## Running bootstraps: 100 / 100 ## Calculating bootstrap values... done. # now we can add our bootstrap values plot(nj_rooted, cex = .4) axisPhylo() nodelabels(bootstrap, cex = .7) # now we can collapse some of our nodes that have too low values nj_collapsed &lt;- nj_rooted # step 1: figure out the row numbers of the branches that can be collapsed N &lt;- length(nj_rooted$tip.label) tocollapse &lt;- match(which(bootstrap&lt;65)+N, nj_rooted$edge[,2]) # step 2: get rid of those branches nj_collapsed$edge.length[tocollapse] &lt;- 0 nj_collapsed &lt;- di2multi(nj_collapsed, tol = 0.00001) # step 3: check if it worked by plotting this new tree plot(nj_collapsed, cex = .45) # lets add some nice colours to the plot so we can see the countries more easily nj_tree_withcolours &lt;- ggtree(nj_collapsed) + geom_treescale() metadata$country &lt;- as.factor(metadata$country) nj_tree_withcolours &lt;- nj_tree_withcolours %&lt;+% metadata + geom_tiplab(aes(color = country), size = 2.5) nj_tree_withcolours # we can even zoom in on some parts of the tree zoomed &lt;- ggtree(tree_subset(nj_tree, &quot;MK462258&quot;)) + geom_treescale() zoomed %&lt;+% metadata + geom_tiplab(aes(color = country), size = 2.5) 4.5.3 Making Phylogeny automated Now that I have created something I want to be able to show easily, without all the hassle of recreating this code over and over again, I will implement this into a shiny so its more user friendly. Because hosting a shiny in bookdown is not possible without launching it as an app I will just include the code. The original file can be found at data/phylo.shiny.Rmd. ui &lt;- fluidPage( # makes you choose your own theme (I like themes (: ) shinythemes::themeSelector(), titlePanel(&quot;Create your own phylogenetic tree&quot;), sidebarLayout( sidebarPanel( h4(&quot;Input files&quot;), fileInput(inputId = &quot;fasta_file&quot;, label = &quot;Select a fasta file&quot;, accept = &quot;.fa&quot;), fileInput(inputId = &quot;metadata&quot;, label = &quot;Select the metadata&quot;, accept = &quot;.csv&quot;), actionButton(inputId = &quot;go&quot;, label = &quot;Start analysis/Reset graph&quot;), h4(&quot;Retrieval settings&quot;), textInput(inputId = &quot;accession&quot;, label = &quot;Insert the accession number&quot;), actionButton(inputId = &quot;zoom&quot;, label = &quot;Zoom in on clade&quot;), h4(&quot;Visualization&quot;), selectInput(inputId = &quot;colour&quot;, label = &quot;Which factor should be distinguished on&quot;, choices = c(&quot;country&quot;, &quot;host&quot;, &quot;isolation&quot;, &quot;collection_date&quot;), selected = &quot;country&quot;), width = 3 ), mainPanel( tabsetPanel( tabPanel(&quot;How to download the required files&quot;, tags$div( tags$h4(&quot;Requirements&quot;), &quot;This shiny requires very specific data files, so to make sure there isnt any confusion I made a list of what these files need:&quot;, tags$br(), &quot;1. All sequences have to be the same length.&quot;, tags$br(), &quot;2. Metadata must contain \\&quot;accession\\&quot;, \\&quot;host\\&quot;, \\&quot;country\\&quot;, \\&quot;isolation\\&quot; and \\&quot;collection_date\\&quot;.&quot;, tags$br(), tags$br(), tags$h4(&quot;Downloading files&quot;), &quot;1. Go to the site of &quot;, tags$a(href=&quot;https://www.ncbi.nlm.nih.gov/genome/viruses/variation/&quot;, &quot;the NCBI.&quot;), tags$br(), &quot;2. Go to the virus databank you want to make the tree of.&quot;, tags$br(), &quot;3. Select nucleotide sequence type.&quot;, tags$br(), &quot;4. Select your filters. (tip: select a region that has the full cds)&quot;, tags$br(), &quot;5. Click \\&quot;Add query\\&quot; and \\&quot;Show results\\&quot;.&quot;, tags$br(), &quot;6. Make sure you only select the same region with the same length!!&quot;, tags$br(), &quot;7. Click \\&quot;Customize label\\&quot;, make sure the label only contains \\&quot;{accession}\\&quot;. (this makes it easier to see later on) &quot;, tags$br(), &quot;8. Download the \\&quot;Nucleotide (fasta)\\&quot; and \\&quot;Result set (CSV)\\&quot; options.&quot;, tags$br(), &quot;9. Input these two files in this shiny and go to the next tab.&quot;, tags$br(),tags$br(), tags$h4(&quot;Extra information&quot;), &quot;Creating a phylogeny tree is not easy and requires allot of factors for it to go right, thats why this shiny only has the basics. Later on I am planning on adding \\&quot;Multiple Sequence Alignment\\&quot;, but I do not have time for that at this moment.&quot;, tags$br(), &quot;If there are any questions feel free to contact me through&quot;, tags$a(href=&quot;https://github.com/thijmenvanbrenk&quot;, &quot;my github&quot; ))), tabPanel(&quot;Phylogenetic tree&quot;, tableOutput(&quot;meta&quot;), plotOutput(&quot;phylo&quot;)) ), width = 9) ) ) This is what the ui looks like. server &lt;- function(input, output, session) { # inputting the fasta file nucleotides &lt;- reactive({ req(input$fasta_file) readDNAStringSet(input$fasta_file$datapath) }) # inputting the metadata file metadata &lt;- reactive({ req(input$metadata) read.csv(input$metadata$datapath) }) # extracting just the years from metadata metadata_year_temp &lt;- reactive({ metadata() %&gt;% separate(collection_date, into = &quot;collection_date&quot;, sep = 4) }) # making inputted tree nj_tree &lt;- reactive({ as.DNAbin(nucleotides()) %&gt;% dist.dna(model = &quot;TN93&quot;) %&gt;% bionj() }) # time to make the graph # first create te full graph tree_foundation &lt;- reactive({ ggtree(nj_tree()) + geom_treescale() }) tree_full &lt;- reactive({ tree_foundation() %&lt;+% metadata_year_temp() + geom_tiplab(aes_string(color = input$colour), size = 3) }) # then the zoomed in graph tree_zoomed &lt;- reactive({ ggtree(tree_subset(nj_tree(), input$accession)) + geom_treescale() }) tree_zoomed_full &lt;- reactive({ tree_zoomed() %&lt;+% metadata_year_temp() + geom_tiplab(aes_string(color = input$colour), size = 3) }) # now tell shiny which to show when tree_output &lt;- reactiveValues(plot=NULL) observeEvent(input$go, { tree_output$plot &lt;- tree_full() }) observeEvent(input$zoom, { tree_output$plot &lt;- tree_zoomed_full() }) # now we can output the plot output$phylo &lt;- renderPlot({ if(is.null(tree_output$plot)) { } else { tree_output$plot } }) # show the information about the selected accession code output$meta &lt;- renderTable({ output_table &lt;- metadata_year_temp() %&gt;% filter(metadata_year_temp()$accession == input$accession) if(nrow(output_table) == 0) { validate(&quot;This accession number does not exist&quot;) } else { output_table } }) } shinyApp(ui, server, options = list(height=650, width = 1300)) And these are pictures of the server in action, it can create graphs like these. References "],["references.html", "Section 5 References", " Section 5 References "]]
